{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:19:37.265555300Z",
     "start_time": "2023-12-19T13:19:34.241514900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "     author_id  label                                             tokens\n0  t2_2hrxxs28      0  ['question', ',', 'doctor', ',', 'how', \"'d\", ...\n1  t2_2hrxxs28      0  ['butt', 'covid', '+', 'cycle', '.', 'i', \"'m\"...\n2  t2_2hrxxs28      0  ['different', 'doctors', '.', 'situation', 'su...\n3   t2_4pxpgwz      0  ['thought', 'pebbleyeet', 'guy', 'autistic', '...\n4   t2_4pxpgwz      0  ['…', 'i', 'always', 'end', 'voting', 'wrong',...\n5   t2_4pxpgwz      0  ['made', 'feel', 'lot', 'better', '.', 'ooh', ...\n6   t2_4pxpgwz      0  ['mouth', ',', 'you', '’d', 'panic', 'attack',...\n7   t2_4pxpgwz      0  ['did', 'nt', 'read', 'top', 'half', 'bc', 'cr...\n8   t2_4pxpgwz      0  ['hot', '?', 'ca', 'n’t', 'much', ',', 'either...\n9   t2_4pxpgwz      0  ['otherwise', ',', 'though', ',', '“', 'needin...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author_id</th>\n      <th>label</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t2_2hrxxs28</td>\n      <td>0</td>\n      <td>['question', ',', 'doctor', ',', 'how', \"'d\", ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t2_2hrxxs28</td>\n      <td>0</td>\n      <td>['butt', 'covid', '+', 'cycle', '.', 'i', \"'m\"...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t2_2hrxxs28</td>\n      <td>0</td>\n      <td>['different', 'doctors', '.', 'situation', 'su...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['thought', 'pebbleyeet', 'guy', 'autistic', '...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['…', 'i', 'always', 'end', 'voting', 'wrong',...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['made', 'feel', 'lot', 'better', '.', 'ooh', ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['mouth', ',', 'you', '’d', 'panic', 'attack',...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['did', 'nt', 'read', 'top', 'half', 'bc', 'cr...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['hot', '?', 'ca', 'n’t', 'much', ',', 'either...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['otherwise', ',', 'though', ',', '“', 'needin...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/tokenized_extrovert.csv', engine='pyarrow')\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T22:42:17.468291100Z",
     "start_time": "2023-12-18T22:42:16.255604400Z"
    }
   },
   "id": "19710e4021845326"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will of course keep numbers for the model, but having only text in this notebook makes it easy to visualize"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e513922cebc2eba7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "remove_int = False\n",
    "\n",
    "def remove_integers(tokens):\n",
    "    if any(char.isdigit() for char in tokens):\n",
    "        return ''\n",
    "    else:\n",
    "        return tokens\n",
    "if remove_int:\n",
    "    df['tokenized'] = df['tokenized'].apply(remove_integers)\n",
    "    df = df[df['tokenized'] != '']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T22:42:24.551363Z",
     "start_time": "2023-12-18T22:42:24.534758Z"
    }
   },
   "id": "8f7ffd943233b68d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "df_1 = df[:1]\n",
    "df_10 = df[:10]\n",
    "df_100 = df[:100]\n",
    "df_1000 = df[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:27:18.225447Z",
     "start_time": "2023-12-18T21:27:18.214917900Z"
    }
   },
   "id": "64544284485ae250"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "tokens_1 = df_1['tokenized'].to_numpy()\n",
    "tokens_10 = df_10['tokenized'].to_numpy()\n",
    "tokens_100 = df_100['tokenized'].to_numpy()\n",
    "tokens_1000 = df_1000['tokenized'].to_numpy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:27:18.246303100Z",
     "start_time": "2023-12-18T21:27:18.227799300Z"
    }
   },
   "id": "991beebdd60697f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorization\n",
    "Mostly from: [the holy bible](https://neptune.ai/blog/vectorization-techniques-in-nlp-guide)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa84f989c8ec070b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorized representation of first 10 rows using _bag-of-words_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7047a2d6ab80abd9"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "x = cv.fit_transform(tokens_10)\n",
    "print(x.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:27:18.299784600Z",
     "start_time": "2023-12-18T21:27:18.232955400Z"
    }
   },
   "id": "4303978c4eae8cca"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00 extroverted', '000 people', '000 pounds', '10 covid', '10 days', '10 minutes', '10 ugh', '10 worse', '100 done', '100 ems']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(cv.vocabulary_.keys())[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:27:18.326287100Z",
     "start_time": "2023-12-18T21:27:18.256475Z"
    }
   },
   "id": "4b427127041afed3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF*iDF\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dab400fc120523c5"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:27:18.326287100Z",
     "start_time": "2023-12-18T21:27:18.263021Z"
    }
   },
   "id": "5f522a307fd61426"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('00', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043517109952789526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])]\n"
     ]
    }
   ],
   "source": [
    "x = tfidf.fit_transform(tokens_100)\n",
    "vectors = x.T.todense()\n",
    "print(sorted(zip(\n",
    "    tfidf.get_feature_names_out(),\n",
    "    vectors[0].tolist()),\n",
    "    key=lambda x: x[1], reverse=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:27:18.406917100Z",
     "start_time": "2023-12-18T21:27:18.268468800Z"
    }
   },
   "id": "df24933b4e2786ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word2Vec\n",
    "[literature](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "[cool app](https://ronxin.github.io/wevi/)\n",
    "[huge thanks to this paper](https://www.analyticsvidhya.com/blog/2023/07/step-by-step-guide-to-word2vec-with-gensim/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b85138dc53a65d4"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:27:18.406917100Z",
     "start_time": "2023-12-18T21:27:18.367162100Z"
    }
   },
   "id": "410504f5542ea0ce"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "sent = [row.split() for row in tokens_1000]\n",
    "\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:27:34.974718Z",
     "start_time": "2023-12-18T21:27:32.686175700Z"
    }
   },
   "id": "342c24b4c9d7b498"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "                  Word  Frequency\n14                '.',      71174\n1                 ',',      62255\n98                '\"',      10336\n7                 '?',       9762\n235               ')',       7197\n...                ...        ...\n26544      'obligate',          1\n26543       'sheerly',          1\n26540  ['spontaneity',          1\n26539         'swing']          1\n46486       'managed']          1\n\n[46487 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14</th>\n      <td>'.',</td>\n      <td>71174</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>',',</td>\n      <td>62255</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>'\"',</td>\n      <td>10336</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>'?',</td>\n      <td>9762</td>\n    </tr>\n    <tr>\n      <th>235</th>\n      <td>')',</td>\n      <td>7197</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26544</th>\n      <td>'obligate',</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26543</th>\n      <td>'sheerly',</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26540</th>\n      <td>['spontaneity',</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26539</th>\n      <td>'swing']</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>46486</th>\n      <td>'managed']</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>46487 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "\n",
    "df_word_freq = pd.DataFrame(list(word_freq.items()), columns=['Word', 'Frequency'])\n",
    "df_word_freq = df_word_freq.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "df_word_freq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:27:37.318899900Z",
     "start_time": "2023-12-18T21:27:36.415913200Z"
    }
   },
   "id": "33774df968be2a8d"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "cores = os.cpu_count()\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sg=1,  # 1 for skip-gram, CBOW otherwise\n",
    "    \n",
    "    min_count=20,\n",
    "    window=2,\n",
    "    sample=6e-5,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    negative=20,\n",
    "    workers=cores-1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:28:18.028125300Z",
     "start_time": "2023-12-18T21:28:18.023741600Z"
    }
   },
   "id": "c0dcc3cec5ef31b7"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b0abdaed12558870"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d74b5cd03784ea6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Some EDA on tokenized words "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70fdb5bca0cb2fec"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "(12504545, 30158460)"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(\n",
    "    sentences,\n",
    "    total_examples=w2v_model.corpus_count,\n",
    "    epochs=30,\n",
    "    total_words=len(sentences),\n",
    "    # report_delay=1\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:44:47.994065500Z",
     "start_time": "2023-12-18T21:44:21.612316300Z"
    }
   },
   "id": "7e63efb4f18773a7"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/5101 is '.',\n",
      "word #1/5101 is ',',\n",
      "word #2/5101 is '\"',\n",
      "word #3/5101 is '?',\n",
      "word #4/5101 is ')',\n",
      "word #5/5101 is 'like',\n",
      "word #6/5101 is '-',\n",
      "word #7/5101 is '...',\n",
      "word #8/5101 is '(',\n",
      "word #9/5101 is 'people',\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(w2v_model.wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(w2v_model.wv.index_to_key)} is {word}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:49:31.386480700Z",
     "start_time": "2023-12-18T21:49:31.379622Z"
    }
   },
   "id": "5723195042c351a0"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'like' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m similar_words \u001B[38;5;241m=\u001B[39m \u001B[43mw2v_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmost_similar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpositive\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlike\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word, similarity \u001B[38;5;129;01min\u001B[39;00m similar_words:\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mword\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msimilarity\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001B[0m, in \u001B[0;36mKeyedVectors.most_similar\u001B[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001B[0m\n\u001B[0;32m    838\u001B[0m         weight[idx] \u001B[38;5;241m=\u001B[39m item[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    840\u001B[0m \u001B[38;5;66;03m# compute the weighted average of all keys\u001B[39;00m\n\u001B[1;32m--> 841\u001B[0m mean \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_mean_vector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_normalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpost_normalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_missing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    842\u001B[0m all_keys \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    843\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_index(key) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m keys \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, _KEY_TYPES) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_index_for(key)\n\u001B[0;32m    844\u001B[0m ]\n\u001B[0;32m    846\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m indexer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(topn, \u001B[38;5;28mint\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001B[0m, in \u001B[0;36mKeyedVectors.get_mean_vector\u001B[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001B[0m\n\u001B[0;32m    516\u001B[0m         total_weight \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mabs\u001B[39m(weights[idx])\n\u001B[0;32m    517\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ignore_missing:\n\u001B[1;32m--> 518\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not present in vocabulary\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    520\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m total_weight \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    521\u001B[0m     mean \u001B[38;5;241m=\u001B[39m mean \u001B[38;5;241m/\u001B[39m total_weight\n",
      "\u001B[1;31mKeyError\u001B[0m: \"Key 'like' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "similar_words = w2v_model.wv.most_similar(positive=[\"like\"])\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:46:50.675957Z",
     "start_time": "2023-12-18T21:46:50.627675200Z"
    }
   },
   "id": "6b3cba4b0ee926fe"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c56f6729157eb179"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
