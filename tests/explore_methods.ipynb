{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-21T13:41:18.446009300Z",
     "start_time": "2023-12-21T13:41:14.524871500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "     author_id  label                                             tokens\n0  t2_2hrxxs28      0  ['question', ',', 'doctor', ',', 'how', \"'d\", ...\n1  t2_2hrxxs28      0  ['butt', 'covid', '+', 'cycle', '.', 'i', \"'m\"...\n2  t2_2hrxxs28      0  ['different', 'doctors', '.', 'situation', 'su...\n3   t2_4pxpgwz      0  ['thought', 'pebbleyeet', 'guy', 'autistic', '...\n4   t2_4pxpgwz      0  ['…', 'i', 'always', 'end', 'voting', 'wrong',...\n5   t2_4pxpgwz      0  ['made', 'feel', 'lot', 'better', '.', 'ooh', ...\n6   t2_4pxpgwz      0  ['mouth', ',', 'you', '’d', 'panic', 'attack',...\n7   t2_4pxpgwz      0  ['did', 'nt', 'read', 'top', 'half', 'bc', 'cr...\n8   t2_4pxpgwz      0  ['hot', '?', 'ca', 'n’t', 'much', ',', 'either...\n9   t2_4pxpgwz      0  ['otherwise', ',', 'though', ',', '“', 'needin...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author_id</th>\n      <th>label</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t2_2hrxxs28</td>\n      <td>0</td>\n      <td>['question', ',', 'doctor', ',', 'how', \"'d\", ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t2_2hrxxs28</td>\n      <td>0</td>\n      <td>['butt', 'covid', '+', 'cycle', '.', 'i', \"'m\"...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t2_2hrxxs28</td>\n      <td>0</td>\n      <td>['different', 'doctors', '.', 'situation', 'su...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['thought', 'pebbleyeet', 'guy', 'autistic', '...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['…', 'i', 'always', 'end', 'voting', 'wrong',...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['made', 'feel', 'lot', 'better', '.', 'ooh', ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['mouth', ',', 'you', '’d', 'panic', 'attack',...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['did', 'nt', 'read', 'top', 'half', 'bc', 'cr...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['hot', '?', 'ca', 'n’t', 'much', ',', 'either...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>t2_4pxpgwz</td>\n      <td>0</td>\n      <td>['otherwise', ',', 'though', ',', '“', 'needin...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/tokenized_extrovert.csv', engine='pyarrow')\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T13:41:19.896313900Z",
     "start_time": "2023-12-21T13:41:18.447592400Z"
    }
   },
   "id": "19710e4021845326"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40452 entries, 0 to 40451\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   author_id  40452 non-null  object\n",
      " 1   label      40452 non-null  int64 \n",
      " 2   tokens     40452 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 948.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T14:32:31.233210400Z",
     "start_time": "2023-12-21T14:32:31.200099700Z"
    }
   },
   "id": "ff1a1e1ef5aff74c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will of course keep numbers for the model, but having only text in this notebook makes it easy to visualize"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e513922cebc2eba7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "remove_int = False\n",
    "\n",
    "def remove_integers(tokens):\n",
    "    if any(char.isdigit() for char in tokens):\n",
    "        return ''\n",
    "    else:\n",
    "        return tokens\n",
    "if remove_int:\n",
    "    df['tokens'] = df['tokens'].apply(remove_integers)\n",
    "    df = df[df['tokens'] != '']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T13:41:19.905723800Z",
     "start_time": "2023-12-21T13:41:19.897632100Z"
    }
   },
   "id": "8f7ffd943233b68d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df_1 = df[:1]\n",
    "df_10 = df[:10]\n",
    "df_100 = df[:100]\n",
    "df_1000 = df[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T13:41:19.905723800Z",
     "start_time": "2023-12-21T13:41:19.901504600Z"
    }
   },
   "id": "64544284485ae250"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tokens_1 = df_1['tokens'].to_numpy()\n",
    "tokens_10 = df_10['tokens'].to_numpy()\n",
    "tokens_100 = df_100['tokens'].to_numpy()\n",
    "tokens_1000 = df_1000['tokens'].to_numpy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T13:41:38.416283Z",
     "start_time": "2023-12-21T13:41:38.394420900Z"
    }
   },
   "id": "991beebdd60697f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorization\n",
    "Mostly from: [the holy bible](https://neptune.ai/blog/vectorization-techniques-in-nlp-guide)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa84f989c8ec070b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorized representation of first 10 rows using _bag-of-words_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7047a2d6ab80abd9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "x = cv.fit_transform(tokens_10)\n",
    "print(x.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T13:41:40.743709600Z",
     "start_time": "2023-12-21T13:41:40.709109500Z"
    }
   },
   "id": "4303978c4eae8cca"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00 extroverted', '000 people', '000 pounds', '10 covid', '10 days', '10 minutes', '10 ugh', '10 worse', '100 done', '100 ems']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(cv.vocabulary_.keys())[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T13:41:42.638861300Z",
     "start_time": "2023-12-21T13:41:42.626078Z"
    }
   },
   "id": "4b427127041afed3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF*iDF\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dab400fc120523c5"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TfidfVectorizer' from 'dask_ml.feature_extraction.text' (C:\\Users\\atabekis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_ml\\feature_extraction\\text.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mdd\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marray\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mda\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask_ml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TfidfVectorizer\n\u001B[0;32m      5\u001B[0m tfidf \u001B[38;5;241m=\u001B[39m TfidfVectorizer()\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'TfidfVectorizer' from 'dask_ml.feature_extraction.text' (C:\\Users\\atabekis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_ml\\feature_extraction\\text.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask_ml.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T15:01:00.580703500Z",
     "start_time": "2023-12-21T15:00:56.656038500Z"
    }
   },
   "id": "5f522a307fd61426"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "     author_id                                               post  label\n0  t2_2hrxxs28  question, doctor, how'd get painkillers? otc p...      0\n1  t2_2hrxxs28  butt covid + cycle. i'm sure what's going i've...      0\n2  t2_2hrxxs28  different doctors. situation sucks relate peop...      0\n3   t2_4pxpgwz  thought pebbleyeet guy autistic guy wants “fix...      0\n4   t2_4pxpgwz  …i always end voting wrong even crewmate. hour...      0\n5   t2_4pxpgwz  made feel lot better. ooh yikes half comments ...      0\n6   t2_4pxpgwz  mouth, you’d panic attack whenever tried eat n...      0\n7   t2_4pxpgwz  didnt read top half bc cropped off, thank much...      0\n8   t2_4pxpgwz  hot? can’t much, either strip nude rip skin gu...      0\n9   t2_4pxpgwz  otherwise, though, “needing” masturbation thin...      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author_id</th>\n      <th>post</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t2_2hrxxs28</td>\n      <td>question, doctor, how'd get painkillers? otc p...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t2_2hrxxs28</td>\n      <td>butt covid + cycle. i'm sure what's going i've...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t2_2hrxxs28</td>\n      <td>different doctors. situation sucks relate peop...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t2_4pxpgwz</td>\n      <td>thought pebbleyeet guy autistic guy wants “fix...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t2_4pxpgwz</td>\n      <td>…i always end voting wrong even crewmate. hour...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>t2_4pxpgwz</td>\n      <td>made feel lot better. ooh yikes half comments ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>t2_4pxpgwz</td>\n      <td>mouth, you’d panic attack whenever tried eat n...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>t2_4pxpgwz</td>\n      <td>didnt read top half bc cropped off, thank much...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>t2_4pxpgwz</td>\n      <td>hot? can’t much, either strip nude rip skin gu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>t2_4pxpgwz</td>\n      <td>otherwise, though, “needing” masturbation thin...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = pd.read_csv('../data/cleaned_extrovert.csv', engine='pyarrow')\n",
    "ddf = dd.from_pandas(df_cleaned, npartitions=1000)\n",
    "ddf.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T14:56:01.261032100Z",
     "start_time": "2023-12-21T14:55:58.794460800Z"
    }
   },
   "id": "5f20ea3493fad48e"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 154. GiB for an array with shape (40452, 512176) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m tfidf_matrix \u001B[38;5;241m=\u001B[39m tfidf\u001B[38;5;241m.\u001B[39mfit_transform(ddf[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m----> 2\u001B[0m tfidf_array \u001B[38;5;241m=\u001B[39m da\u001B[38;5;241m.\u001B[39mcompute(\u001B[43mtfidf_matrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001B[0m, in \u001B[0;36m_cs_matrix.toarray\u001B[1;34m(self, order, out)\u001B[0m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m order \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1049\u001B[0m     order \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_swap(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcf\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m-> 1050\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_toarray_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (out\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mc_contiguous \u001B[38;5;129;01mor\u001B[39;00m out\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mf_contiguous):\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOutput array must be C or F contiguous\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001B[0m, in \u001B[0;36m_spbase._process_toarray_args\u001B[1;34m(self, order, out)\u001B[0m\n\u001B[0;32m   1265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[0;32m   1266\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype, order\u001B[38;5;241m=\u001B[39morder)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 154. GiB for an array with shape (40452, 512176) and data type float64"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf.fit_transform(ddf['post'])\n",
    "tfidf_array = da.compute(tfidf_matrix.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T14:56:54.073865100Z",
     "start_time": "2023-12-21T14:56:01.267056900Z"
    }
   },
   "id": "df24933b4e2786ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf_array"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbdcdfe1388b0915"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word2Vec\n",
    "[literature](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "[cool app](https://ronxin.github.io/wevi/)\n",
    "[huge thanks to this paper](https://www.analyticsvidhya.com/blog/2023/07/step-by-step-guide-to-word2vec-with-gensim/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b85138dc53a65d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T13:41:21.262300900Z"
    }
   },
   "id": "410504f5542ea0ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sent = [row.split() for row in tokens_1000]\n",
    "\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T13:41:21.268513400Z",
     "start_time": "2023-12-21T13:41:21.265307600Z"
    }
   },
   "id": "342c24b4c9d7b498"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "\n",
    "df_word_freq = pd.DataFrame(list(word_freq.items()), columns=['Word', 'Frequency'])\n",
    "df_word_freq = df_word_freq.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "df_word_freq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T13:41:21.268513400Z"
    }
   },
   "id": "33774df968be2a8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cores = os.cpu_count()\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sg=1,  # 1 for skip-gram, CBOW otherwise\n",
    "    \n",
    "    min_count=20,\n",
    "    window=2,\n",
    "    sample=6e-5,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    negative=20,\n",
    "    workers=cores-1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T13:41:21.268513400Z"
    }
   },
   "id": "c0dcc3cec5ef31b7"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b0abdaed12558870"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T13:41:21.268513400Z"
    }
   },
   "id": "9d74b5cd03784ea6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Some EDA on tokenized words "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70fdb5bca0cb2fec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(\n",
    "    sentences,\n",
    "    total_examples=w2v_model.corpus_count,\n",
    "    epochs=30,\n",
    "    total_words=len(sentences),\n",
    "    # report_delay=1\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T13:41:21.268513400Z"
    }
   },
   "id": "7e63efb4f18773a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for index, word in enumerate(w2v_model.wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(w2v_model.wv.index_to_key)} is {word}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T13:41:21.277556200Z"
    }
   },
   "id": "5723195042c351a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_words = w2v_model.wv.most_similar(positive=[\"like\"])\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T13:41:21.278764500Z"
    }
   },
   "id": "6b3cba4b0ee926fe"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c56f6729157eb179"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
